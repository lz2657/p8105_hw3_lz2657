---
title: "p8105_hw3_lz2657"
author: Lingyu Zhang
date: Oct 7, 2018
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::install_github("p8105/p8105.datasets", force = TRUE)
library(tidyverse)
library(readxl)
library(dplyr)
library(p8105.datasets)
library(ggplot2)
```

## Problem 1

First, read and manipulate the data as required.

```{r problem1_read_clean}
data("brfss_smart2010")
brfss_data = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response == "Excellent" | response =="Very good" | response == "Good" | response == "Fair" | response =="Poor")
brfss_data$response = factor(brfss_data$response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))

brfss_data
```

Question 1:
```{r problem1_question1}
count_states_2002 = count(count(subset(brfss_data, year == 2002), locationabbr, locationdesc), locationabbr)
filter(count_states_2002, nn == 7)
```

In 2002, only CT, FL and NC were observed at 7 locations.

Question 2:
```{r problem1_question2, fig.width = 10, fig.height = 8}
count_states_year = count(count(brfss_data, year, locationabbr, locationdesc), year, locationabbr)

ggplot(data = count_states_year, aes(x = year, y = nn, colour = locationabbr)) +
labs(list(title = "year - number of locations", y = paste("number of locations"))) + 
geom_line(size=0.5)
```

Here is the required spaghetti plot and I enlarge the size of it to make it clearer.

Question 3:
```{r problem1_question3}
brfss_exclt_prop = subset(brfss_data, locationabbr == "NY" & response == "Excellent" & (year == 2002 | year == 2006 | year == 2010)) %>%
  group_by(year) %>%
  summarize(mean = mean(data_value), sd = sd(data_value))

brfss_exclt_prop
```

The mean of the proportion of "Excellent" doensn't have large changes while the sd of it was decreasing.

Question 4:
```{r problem1_question4, fig.width = 13}
brfss_prop = brfss_data %>%
  group_by(year, locationabbr, response) %>%
  summarize(mean = mean(data_value))
ggplot(subset(brfss_prop, !is.na(mean)), aes(x = year, y = mean, color = response)) + 
  geom_smooth(se = FALSE) + 
  facet_grid(. ~ response)
```

The plot is shown above. The proportion of "Very good" is the highest and that of "Poor" is the lowest. All the proportions of the responses other than "Excellent" were almost unchange. The proportion of "Excellent" had decreased for about 2 percent during the period.

## Problem 2

```{r problem2_read, include=FALSE}
data("instacart")
instacart_data = instacart
```

The instacart dataset is a dataframe with 1384617 observations of 15 variables. The variables include order_id <int>, product_id <int>, user_id <int>, , order_dow <int>, order_hour_of_day <int>, , product_name <chr>, aisle_id <int>, aisle <chr> and so on. Order_dow means the day of the week on which the order was placed. Order_number means the order sequence number for this user (1 = first, n = nth). Order_hour_of_day means the hour of the day on which the order was placed. The other variables are just like the name of them. An example of the observations is `r head(instacart_data,1)`, which shows the details of a product from an order.

Question 1:
```{r problem2_question1}
n_distinct(instacart_data$aisle_id)
count_aisle_item = instacart_data %>%
  group_by(aisle_id, aisle) %>%
  summarize(items_number = n())
count_aisle_item[which.max(count_aisle_item$items_number),2]
max(count_aisle_item$items_number)
```

There are 134 aisles and most items are ordered from fresh vegetables, which has 150609 items.

Question 2:
```{r problem2_question2, fig.height = 16}
ggplot(count_aisle_item, aes(x = reorder(aisle_id, -items_number), y = items_number)) + 
  labs(list(x = "aisle id")) +
  geom_bar(stat = "identity") + 
  coord_flip()
```

The plot is shown above. I use bar plot and rank the aisles to make it more readable. Also, I set the height to 16 and flip the coordinate axes so that the id of the aisles can be distinguished.

Question 3:
```{r problem2_question3}
most_pop_item = subset(instacart_data, aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name) %>%
  summarize(items_number = n()) %>%
  group_by(aisle) %>%
  filter(max(items_number) == items_number)

most_pop_item
```

The most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits” are "Light Brown Sugar", "Snack Sticks Chicken & Rice Recipe Dog Treats" and "Organic Baby Spinach", respectively.

Question 4:
```{r problem2_question4}
product_mean_hour = subset(instacart_data, product_name == "Pink Lady Apples" | product_name =="Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour= mean(order_hour_of_day)) %>%
  spread(key = product_name, value = mean_hour)

product_mean_hour
```

Here is the required table. It turns out that the mean hours of Coffee Ice Cream are almost all longer than that of Pink Lady Apples on each day of the week.

## Problem 3

```{r problem3_read, include=FALSE}
data("ny_noaa")
data_noaa = ny_noaa
```

The ny_noaa data is a dataframe with 2595176 observations of 7 variables. The variables are id <chr>, date <date>, prcp <int>, snow <int>, snwd <int>, tmax <chr> and tmin<chr>, which mean weather station ID, date of observation, precipitation (tenths of mm), snowfall (mm), snow depth (mm), maximum and minimum temperature (tenths of degrees C), respectively. The proportions of the missing data in five key variables are `r round(sum(is.na(ny_noaa$prcp))/2595176 * 100, 2)`%, `r round(sum(is.na(ny_noaa$snow))/2595176 * 100, 2)`%, `r round(sum(is.na(ny_noaa$snwd))/2595176 * 100, 2)`%, `r round(sum(is.na(ny_noaa$tmax))/2595176 * 100, 2)`% and `r round(sum(is.na(ny_noaa$tmin))/2595176 * 100, 2)`%.

```{r problem3_clean}
noaa_data = ny_noaa %>%
  separate(date, into = c("year","month","day"), sep = '-') %>%
  gather(key = type, value = data, prcp:tmin, na.rm = TRUE) %>%
  transform(data = as.numeric(data))
```

Question 1:
```{r problem3_question1}
noaa_snow = subset(noaa_data, type == "snow")
noaa_snow_count = count(noaa_snow,data)
filter(noaa_snow_count, n == max(n))
```

The most commonly observed values for snowfall is 0, which is observed by 2008508 times. This is because it's not snowy in NY most of the time.

Question 2:
```{r problem3_question2}
noaa_tmax_jan_jul = subset(noaa_data, type == "tmax" & (month == "01" | month == "07")) %>%
  group_by(year, month) %>%
  summarize(mean_tmax = mean(data))

ggplot(noaa_tmax_jan_jul, aes(x = year, y = mean_tmax, color = month)) + 
  geom_bar(stat = "identity") + 
  scale_x_discrete(breaks = seq(1981, 2010, by = 5)) +
  facet_grid(. ~ month)
```

The plot is shown above. The means of tmax in January concentrate around -2 and those in July concentrate around 270. There is no outliers in both of the data since each one locates in [Q1 - 1.5 IQR, Q3 + 1.5 IQR].